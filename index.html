<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>VIG</title>

  <!-- Meta tags for social sharing (optional) -->
  <meta name="description" content="DESCRIPTION META TAG" />
  <meta property="og:title" content="ReGuide | ICLR 2025" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:title" content="ReGuide | ICLR 2025" />
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG" />
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  <!-- Favicon and Fonts -->
  <link rel="icon" type="image/svg+xml" href="static/images/icon.svg" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.css" />

  <!-- Style -->
  <style>
    /* 슬라이더 전체의 최소 높이 설정 (글자가 튀는 현상 방지) */
    .mySwiper {
      padding-bottom: 50px; /* 하단 페이지네이션 공간 */
    }
  
    .swiper-slide {
      display: flex;
      flex-direction: column;
      justify-content: flex-start; /* 위에서부터 차례대로 배치 */
      align-items: center;
      height: auto;
      min-height: 550px; /* 가장 긴 슬라이드 기준으로 높이 조절 */
    }
  
    /* 이미지가 들어가는 박스 높이를 고정해서 텍스트 시작 위치를 맞춤 */
    .slide-image-container {
      display: flex;
      justify-content: center;
      align-items: center;
      width: 100%;
      height: 300px; /* 모든 슬라이드의 이미지 영역 높이를 통일 */
      margin-bottom: 1.5rem;
    }
  
    .slide-image-container img {
      max-width: 100%;
      max-height: 100%; /* 박스 높이를 넘지 않게 함 */
      width: auto;
      height: auto;
      object-fit: contain; /* 비율 유지하며 박스 안에 맞춤 */
    }
  
    /* 텍스트 컨텐츠는 왼쪽 정렬 유지 (가독성) */
    .swiper-slide .content {
      width: 100%;
      text-align: left;
      padding: 0 1rem;
    }
  </style>
  
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Selective Training for Large Vision Language Models via Visual Information Gain</h1>
            <h2 class="title is-3">arXiv</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/seulbi-lee-924004342/">Seulbi Lee</a>,
                <a href="https://www.linkedin.com/in/sangheum-hwang-99a957a3/">Sangheum Hwang</a>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Seoul National University of Science and Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="arXiv 링크 넣기" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/daintlab/VIG-training" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Github</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/daintlab/reguide" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/huggingface.png" alt="Hugging Face Logo" style="height: 1em; vertical-align: middle;">
                    </span>
                    <span>Huggingface</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<hr>
  
  <section class="section abstract">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          
          <section class="section">
            <div class="container is-max-desktop">
              <div class="columns is-vcentered">
                <div class="column has-text-centered">
                  <figure class="image">
                    <img src="static/images/irrel-sample.png" alt="Radar All" style="width: 120%; height: auto;">
                    <figcaption>
                      <p><small>(a) A sample that can be answered from common sense </small></p>
                    </figcaption>
                  </figure>
                </div>
                <div class="column has-text-centered">
                  <figure class="image">
                    <img src="static/images/rel-sample.png" alt="Radar ReGuide" style="width: 120%; height: auto;">
                    <figcaption>
                      <p><small>(b) A sample that requires fine-grained visual understanding </small></p>
                    </figcaption>
                  </figure>
                </div>
              </div>
            </div>
          </section>

          <div class="content has-text-justified">
            <!-- <p><strong>Visual Information Gain</strong> is a perplexity-based metric that quantifies the contribution of visual input by measuring the reduction in model uncetainty.
            </p> -->
            <br>
            <strong>Motivation</strong>
            <ul>
              <li><strong>Language Bias in LVLMs</strong>: Large Vision-Language Models often ignore visual evidence, relying instead on learned linguistic priors.
              <li><strong>Data Heterogeneity Gap</strong>: Instruction datasets are a mixture of examples, ranging from those solvable by linguistic context alone to those demanding genuine visual understanding.</li>
              <li><strong>Uniform Training</strong>: Uniform training on this heterogeneous data encourages models to exploit shortcuts rather than learning robust visual grounding.</li>
            </ul>

            <strong>Research Question</strong>
            <ul>
              <li><strong>Quantification</strong>: <em>Can we directly <strong>measure</strong> how much each training sample and token <strong>benefits from the visual input?</strong></em></li>
              <li><strong>Optimization</strong>: <em>Can we use this signal to <strong>focus</strong> learning on genuinely visual evidance?</em></li>
            </ul>

            <strong>Method</strong>
            <ul>
              <li><strong>Visual Information Gain (VIG)</strong>: We introduce a perplexity-based metric that quantifies the contribution of visual input.</li>
              <li><strong>VIG as a Grounding Indicator</strong>: We analyse that VIG reliably serves as an indicator of visual grounding across sample, benchmark, token level.
              <li><strong>VIG-Guided Selective Training</strong>: We propose a strategy that prioritizes high VIG-samples and tokens.</li>
            </ul>

            <strong>Results</strong>
            <ul>
              <li><strong>Enhanced Data Efficiency</strong>: We achieve superior performance using only a fraction of the data by pruning weakly grounded samples.</li>
              <li><strong>Improved Visual Grounding</strong>: Our method significantly improves performance on various benchmarks by focusing optimization on visual tokens.</li>
              <li><strong>Mitigated Language Bias</strong>: The VIG-guided approach successfully reduces linguistic bias.</li>
            </ul>
            </div>
          </div>
        </div>
      </div>
    </section>


  <hr>
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Visual Information Gain</h2>
      <!-- framework -->
      <h4 class="title is-4">Definition</h4>
      <div style="display: flex; justify-content: center; align-items: center; width: 100%; margin-bottom: 1rem;">
        <img src="static/images/vig-equation.png" alt="Run 1" style="width: 70%; height: auto;">
      </div>
      <div class="content" style="text-align: left;">
        <ul>
          <li>We introduce VIG, which quantifies how much the inclusion of image $I$ reduces the model's uncertainty.</li>
          <li>VIG is defined as the log-ratio between the model's perplexities on the same answer $A$ with and without visual conditioning.</li>
          <li>Based on the relationship $PPL = \exp(\mathcal{L})$, where $\mathcal{L}$ is the cross-entropy loss, VIG inherently reflects the aggregate <em>contribution of per-token loss reductions.</em></li>
        </ul>
      </div>
      <h4 class="title is-4">Analyses</h4>
      <div class="container is-max-desktop">
        <div class="swiper mySwiper">
          <div class="swiper-wrapper">
            <div class="swiper-slide">
              <div style="display: flex; justify-content: center; align-items: center; width: 100%; margin-bottom: 1rem;">
                <img src="static/images/sample-level.png" alt="Run 1" style="width: 70%; height: auto;">
              </div>
              <div class="content" style="text-align: left;">
                <p><strong>Sample-level</strong>: VIG is a fine-grained measure of visual grounding. </p>
                <ul>
                  <li>Image 1: high positive VIG - full alignment between image and text.</li>
                  <li>Image 2: moderately positive VIG - correct main subject, but attribute mismatches.</li>
                  <li>Image 3: negative VIG - visual content increases uncertainty and conflicts with the text.</li>              
                </ul>
              </div>
            </div>
            <div class="swiper-slide">
              <div style="display: flex; justify-content: center; align-items: center; width: 100%; margin-bottom: 1rem;">
                <img src="static/images/benchmark-level.png" alt="Run 2" style="width: 60%; height: auto;">
              </div>
              <div class="content" style="text-align: left;">
                <p><strong>Benchmark-level</strong>: VIG aligns with benchmark-level modality dependency.</p>
                <ul>
                  <li>COCO exhibits a distribution clearly shifted towards positive values.</li>
                  <li>POPE presents a distribution centered near zero but still slightly biased toward the positive side.</li>
                  <li>GQA and SQA show distributions with negative mean scores.</li>
                </ul>
              </div> 
            </div>
            <div class="swiper-slide">
              <section class="section">
                <div class="container is-max-desktop">
                  <div class="columns is-vcentered">
                    <div class="column has-text-centered">
                      <figure class="image">
                        <img src="static/images/token-level-scatter.png" alt="Radar All" style="width: 80%; height: auto;">
                        <figcaption>
                          <p><small>(a) Visualizing the token-level VIGs </small></p>
                        </figcaption>
                      </figure>
                    </div>
                    <div class="column has-text-centered">
                      <figure class="image">
                        <img src="static/images/token-level-table.png" alt="Radar ReGuide" style="width: 120%; height: auto;">
                        <figcaption>
                          <p><small>(b) Tokens and their loss differences in LLaVA-1.5 </small></p>
                        </figcaption>
                      </figure>
                    </div>
                  </div>
                </div>
              </section>
              <div class="content" style="text-align: left;">
                <p><strong>Token-level</strong>: VIG captures token-level visual grounding.</p>
                <ul>
                  <li>
                    The tokens in the red region represent prediction losses are substantially reduced by image. (strong visual grounding)
                  </li>
                  <li>
                    The tokens in the blue region primarily serve as functional roles. (weak visual contribution)
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
          <!-- Navigation arrows -->
          <div class="swiper-button-next"></div>
          <div class="swiper-button-prev"></div>
    
          <!-- Pagination dots (optional) -->
          <div class="swiper-pagination"></div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">VIG-guided Selective Training</h2>
      <h4 class="title is-4">Framework</h4>
      <div class="content">
        <p>
          VIG-guided selective training is the practical training strategy that trains the model by focusing only on the <em>"part that need to be seen"</em> of the <em>"data that needs to be seen"</em>
        </p>
        <br>
        <ol>
          <li>1. All training samples are ranked by VIG and the top $p%$ are selected.</li>
          <li>2. Within this curated subset, token-level selection is further performed using same threshold $\tau_p$</li>
          <li>3. The model is optimized using only the selected tokens.</li>
        </ol>
      </div>
      <!-- result -->
      <h4 class="title is-4">Experimental Results</h4>
      <div style="width:100%; margin: 0 auto 1rem auto; text-align: center;">
        <figure style="display: inline-block;">
          <img src="static/images/result-reguide.png" alt="OoDD in LVLMs" style="max-width: 100%; height: auto;">
        </figure>
      </div>
      <div class="content">
        <strong>General Performance</strong>
          <ul>
            <li>ReGuide improves both ID classification and OoDD performance, especially when using visually similar classes (i.e., near-OoD).</li>
            <li>Suggesting similar classes helps models better distinguish fine-grained differences between ID inputs.</li>
            <li>InternVL2-76B and GPT-4o show gains in AUROC, FPR@95%TPR, and valid response rate when guided by ReGuide.</li>
            <li>GPT-4o benefits more from ReGuide in near-OoD detection, even though it sometimes struggles with ambiguous inputs.</li>
          </ul>
          
          <strong>Insights</strong>
          <ul>
            <li>ReGuide's effectiveness stems from using the image itself to generate auxiliary classes rather than relying on external text.</li>
            <li>This model-agnostic strategy provides relevant and adaptive guidance to LVLMs based on each specific input.</li>
          </ul>
      </div>
    </div>
  </section>

<!--   
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">BibTeX</h2>
      <div style="background: #f5f5f5; padding: 1em; border-radius: 6px; font-family: monospace;">
        @inproceedings{<br>
        &nbsp;&nbsp;kim2025reflexive,<br>
        &nbsp;&nbsp;title={Reflexive Guidance: Improving Oo{DD} in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation},<br>
        &nbsp;&nbsp;author={Jihyo Kim and Seulbi Lee and Sangheum Hwang},<br>
        &nbsp;&nbsp;booktitle={The Thirteenth International Conference on Learning Representations},<br>
        &nbsp;&nbsp;year={2025},<br>
        &nbsp;&nbsp;url={https://openreview.net/forum?id=R4h5PXzUuU}<br>
        }
      </div>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the design of this website, we just ask that you link back to this page in the footer.
              <br />
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>
              .
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.js"></script>
  <script>
  const swiper = new Swiper('.mySwiper', {
    loop: true,
    autoHeight: true, // ✅ Add this line
    navigation: {
      nextEl: '.swiper-button-next',
      prevEl: '.swiper-button-prev',
    },
    pagination: {
      el: '.swiper-pagination',
      clickable: true,
    }
  });
  </script>
  
</body>
</html>
